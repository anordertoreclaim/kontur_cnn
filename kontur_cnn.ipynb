{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kontur_cnn",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MschcMWeLwBq",
        "colab_type": "code",
        "outputId": "efbe090f-e29c-48bf-cecb-879e7059371f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# быстрые токенайзеры\n",
        "!pip install youtokentome"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome) (7.1.1)\n",
            "Installing collected packages: youtokentome\n",
            "Successfully installed youtokentome-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlwG7T-3HxXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import youtokentome as yttm\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jaw_fdS72Nbu",
        "colab_type": "text"
      },
      "source": [
        "Я достаточно долго пытался понять, как было произведено трейн/тест деление. Так как пересечение уникальных русских/английских названий компаний между трейном и тестом очень маленькое (всего 13% русских и 5% английских названий из теста встречаются в трейне), то это явно не случайное (равномерное) разделение.\n",
        "\n",
        "Я сразу подумал о представлении данных в виде двудольного графа: русские имена и английские имена. Если между ними есть ребро, то такая пара в данных встречается, иначе ребра нет. Я пытался загуглить задачу, в которой вершины двудольного графа делят на два непепресекающихся подмножества так, что минимизируется количество ребер между вершинами из разных подмножеств. Все, что я нашел – это один [вопрос](https://math.stackexchange.com/questions/2637808/bipartite-graph-partitioning-special-case?rq=1) на math.stackexchange с задачей с похожей формулировкой, но без ответа. Отсюда можно сделать вывод, что вы не решали ничего похожего.\n",
        "\n",
        "Тогда, по-моему, единственное объяснение – это то, что у вас было больше данных, но некоторую часть ребер (таких, что какая-то его вершина встречаются и в трейне и в тесте) вы удалили, чтобы сделать валидацию более надежной.\n",
        "\n",
        "Так как при случайном разделении вышеупомянутые 13% и 5% превращаются в 90+%, то урезание до нужных процентов привело бы к тому, что данных стало бы заметно меньше. К тому же, я использую два валидационных множества: одно для нахождения границы предсказывания, а другое – для подсчитывания F1 метрики, что также рождает трудности: непонятно, как сделать так, чтобы эти 13 и 5 процентов были как между трейном и первым валидационным множеством, так и между трейном и вторым валидационным множеством, так и между первым и вторым валидационными множествами.\n",
        "\n",
        "Поэтому я принял решение сфокусироваться на моделинге и валидировать на множестве с т.н. \"ликами\" :) Так как пересечение по английским названиям меньше, то я решил делать GroupShuffleSplit по колонке с английскими названиями, т.е. не ликать английские названия в валидационное множество."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXkFho1YmIbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1DGtsGPuDKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('kontur_srs_test_task/train_data.tsv', sep='\\t')\n",
        "train, thresh_split, holdout_split = utils.split_train(train, 'eng_name', thresh_size=0.4, seed=SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB_dFMNlDZn-",
        "colab_type": "text"
      },
      "source": [
        "В качестве модели я использую сиамские сверточные сети с max-пулингом на BPE-токенах, полученных с очищенных данных. Я использую majority-vote ансамбль 3 моделей, натренированных на данных с разными сидами, чтобы задействовать все тренировочные данные. Данные к нижнему регистру я не приводил, так как качество предсказаний после потери регистра заметно падало. Вдохновение я взял с [SentenceBert](https://arxiv.org/abs/1908.10084): там два предложения энкодятся (с mean-пулингом) одним и тем же БЕРТом в вектора u и v, и по конкатенации векторов u, v и |u-v| полносвязный классификатор определяет степень похожести предложений (в моем случае – это бинарный классификатор). Затем на инференсе в качестве показателя похожести предложений берется косинусовая похожесть эмбеддингов предложений.\n",
        "\n",
        "Я быстро определил, что mean-пулинг нужно поменять на max-, и затем изучал влияние гиперпараметров на результат работы модели. Изначально я хотел сделать ансамбль из сиамских ЛСТМок, сиамских трансформеров (ха-ха) и сиамских сверток, но в итоге ЛСТМки показали себя хуже сверток, а смысла в ансамбле сильно отличающихся по качеству моделей нет. Трансформер (энкодер, разумеется), увы, просто отказался учить хоть что-то полезное :(\n",
        "\n",
        "Также я хотел добавить аугментацию данных: например, поиграться с регистрами слов, но не успел по времени.\n",
        "\n",
        "Итак, моя модель – это:\n",
        "\n",
        "\n",
        "*   Эмбеддинг BPE-токенов в 175-мерное пространство (200 для одной из моеделей);\n",
        "*   3 свертки, не меняющих размерность признаков, с окном в 5 и с паддингом;\n",
        "*   ReLu после первой и второй свертки;\n",
        "*   Dropout между первым ReLu и второй сверткой, он же между вторым ReLu и третьей сверткой;\n",
        "*   tanh после третьей свертки для нормализации активаций, затем – глобальный макспулинг;\n",
        "*   через такой пайплайн прогоняются два названия – русское и английское. Пусть их вектора будут u и v соответственно. Конкатенация векторов u, v и |u-v| пропускается через дропаут и подается на вход однослойному бинарному классификатору;\n",
        "*   инференс – получаются вектора u и v, и считается их косинусовая схожесть. Затем по первому валидационному множеству находится трешхолд, который максимизирует F1 метрику, и он же применяется ко второму валидационному множеству. Ну или к тестовому. Все :)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeNImiuKwIA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseCNNEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dims, kernel_sizes, paddings, dropout):\n",
        "        super(SiameseCNNEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=utils.PAD_ID)\n",
        "\n",
        "        convolutional_layers = []\n",
        "        for i in range(len(paddings)):\n",
        "          if i != 0:\n",
        "            convolutional_layers.append(nn.Dropout(dropout))\n",
        "          convolutional_layers.append(nn.Conv1d(hidden_dims[i], \n",
        "                                        hidden_dims[i + 1], \n",
        "                                        kernel_sizes[i], \n",
        "                                        padding=paddings[i]))\n",
        "          if i != len(paddings) - 1:\n",
        "            convolutional_layers.append(nn.ReLU())\n",
        "        \n",
        "        self.convolutions = nn.Sequential(*convolutional_layers)\n",
        "        self.dropout_final = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(3 * hidden_dims[-1], 2)\n",
        "        \n",
        "    def _embed(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.convolutions(x)\n",
        "        x = torch.tanh(x)\n",
        "        x, _ = x.max(dim=2)\n",
        "        return x\n",
        "        \n",
        "    def forward(self, x, y):\n",
        "        x = self._embed(x)\n",
        "        y = self._embed(y)\n",
        "        embedding = torch.cat([x, y, torch.abs(x - y)], axis=-1)\n",
        "        embedding = self.dropout_final(embedding)\n",
        "        logits = self.linear(embedding)\n",
        "        return logits\n",
        "    \n",
        "    def compute_similarity(self, x, y):\n",
        "        x = self._embed(x)\n",
        "        y = self._embed(y)\n",
        "        cosine_similarity = F.cosine_similarity(x, y)\n",
        "        return cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym060oEknksB",
        "colab_type": "text"
      },
      "source": [
        "# Обучение \n",
        "(не обязательно запускать для того, чтобы работал инференс, который ниже)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTs9oaN9LtGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 15\n",
        "BATCH_SIZE = 200\n",
        "\n",
        "VOCAB_SIZE = 320\n",
        "EMBEDDING_DIM = 175\n",
        "HIDDEN_DIMS = [EMBEDDING_DIM, EMBEDDING_DIM, EMBEDDING_DIM, EMBEDDING_DIM]\n",
        "KERNEL_SIZES = [5, 5, 5]\n",
        "DROPOUT = 0.15\n",
        "PADDINGS = [2, 2, 2]\n",
        "\n",
        "LR = 0.01\n",
        "\n",
        "tokenizer_filename = 'tokenizer_320cased_1.model'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Viok8BdVy7aJ",
        "colab_type": "text"
      },
      "source": [
        "Чистка данных: привожу некоторые символы к символам, которые они должны обозначать, а также убираю некоторые символы (те, что не из вайтлиста в регексе)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC2H5s1c0pJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(string):\n",
        "  rep = {'<<': '\"', \n",
        "         '>>': '\"',\n",
        "         '<': '\"', \n",
        "         '>': '\"', \n",
        "         \"''\": '\"', \n",
        "         '``': '\"',\n",
        "         '`': \"'\", \n",
        "         '/': ' ', \n",
        "         '«': '\"', \n",
        "         '»': '\"',\n",
        "         '–': '-',\n",
        "         '_': '-'}\n",
        "  rep = dict((re.escape(k), v) for k, v in rep.items()) \n",
        "\n",
        "  pattern = re.compile(\"|\".join(rep.keys()))\n",
        "  replaced = pattern.sub(lambda m: rep[re.escape(m.group(0))], string)\n",
        "  filtered = ''.join(re.findall(\"[A-Za-z0-9а-яА-Я\\-',.\\(\\)\\\" &@!?\\+\\*№]*\", replaced))\n",
        "  if filtered.strip() == '':\n",
        "    # empty string token\n",
        "    filtered = '#'\n",
        "  return filtered"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTLiQY_ucnNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = utils.transform_dataframe(train, transform)\n",
        "thresh_split = utils.transform_dataframe(thresh_split, transform)\n",
        "holdout_split = utils.transform_dataframe(holdout_split, transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSVlR0TMLY1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "utils.train_bpe_tokenizer(train, tokenizer_filename, VOCAB_SIZE)\n",
        "tokenizer = yttm.BPE(model=tokenizer_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtl91f27zMBm",
        "colab_type": "text"
      },
      "source": [
        "Заранее перевожу строковые данные в id токенов, чтобы не тратить время на токенизацию во время обучения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPPbX9FElgqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_lists = utils.precompute_dataset(train, tokenizer)\n",
        "\n",
        "threshold_lists = utils.precompute_dataset(thresh_split, tokenizer)\n",
        "holdout_lists = utils.precompute_dataset(holdout_split, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JjsxkP5cZXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roJ-qutlxyG6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "61035cd6-12f0-444c-82b8-6fb99100593d"
      },
      "source": [
        "model = SiameseCNNEncoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIMS, KERNEL_SIZES, PADDINGS, DROPOUT)\n",
        "model.to(device)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SiameseCNNEncoder(\n",
              "  (embedding): Embedding(320, 175, padding_idx=0)\n",
              "  (convolutions): Sequential(\n",
              "    (0): Conv1d(175, 175, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.15, inplace=False)\n",
              "    (3): Conv1d(175, 175, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.15, inplace=False)\n",
              "    (6): Conv1d(175, 175, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "  )\n",
              "  (dropout_final): Dropout(p=0.15, inplace=False)\n",
              "  (linear): Linear(in_features=525, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHIsQknIcYBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, nesterov=True)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1, factor=0.2, verbose=True, mode='max')\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcPufVqjwsJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(utils.ListsDataset(*train_lists), shuffle=True, batch_size=BATCH_SIZE, collate_fn=utils.collate_fn)\n",
        "\n",
        "threshold_loader = DataLoader(utils.ListsDataset(*threshold_lists), batch_size=BATCH_SIZE, collate_fn=utils.collate_fn)\n",
        "holdout_loader = DataLoader(utils.ListsDataset(*holdout_lists), batch_size=BATCH_SIZE, collate_fn=utils.collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FN1CRuLwuZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# для нахождения границы предсказывания\n",
        "threshold_tuner = utils.OptimizedRounder(0.73)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JzplSkAwv-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_preds = None\n",
        "best_score = 0.0\n",
        "\n",
        "EPOCHS_WITHOUT_VALID = 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UxHc-5_8G4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# для воспроизводимости\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUfrI2Yg-h4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "  for i, batch in enumerate(tqdm(train_loader, desc='Epoch {}: '.format(epoch))):\n",
        "    ru, eng, labels = batch\n",
        "    ru, eng, labels = ru.to(device), eng.to(device), labels.to(device)\n",
        "    pred = model(ru, eng)\n",
        "    loss = criterion(pred, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if epoch > EPOCHS_WITHOUT_VALID:\n",
        "    similarities = []\n",
        "    trues = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in threshold_loader:\n",
        "        ru, eng, labels = batch\n",
        "        trues.extend(list(labels.numpy()))\n",
        "        ru, eng = ru.to(device), eng.to(device)\n",
        "        similarities.extend(list(model.compute_similarity(ru, eng).cpu().numpy()))\n",
        "\n",
        "    threshold_tuner.fit(similarities, trues)\n",
        "\n",
        "    similarities = []\n",
        "    trues = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in holdout_loader:\n",
        "        ru, eng, labels = batch\n",
        "        trues.extend(list(labels.numpy()))\n",
        "        ru, eng = ru.to(device), eng.to(device)\n",
        "        similarities.extend(list(model.compute_similarity(ru, eng).cpu().numpy()))\n",
        "\n",
        "    def compute_score(tuner):\n",
        "      preds = tuner.predict(similarities)\n",
        "      prec = utils.precision(preds, trues)\n",
        "      rec = utils.recall(preds, trues)\n",
        "      f1 = utils.f_score(preds, trues)\n",
        "      print('Threshold: ', tuner.thresh_) \n",
        "      print('Precision: ', prec)\n",
        "      print('Recall: ', rec)\n",
        "      print('F1 score: ', f1)\n",
        "      return f1\n",
        "\n",
        "    score = compute_score(threshold_tuner)\n",
        "    if score > best_score:\n",
        "      torch.save({\n",
        "        'epoch': epoch,\n",
        "        'score': score,\n",
        "        'seed': SEED,\n",
        "        'vocab_Size': VOCAB_SIZE,\n",
        "        'dropout': DROPOUT,\n",
        "        'embedding_dim': EMBEDDING_DIM,\n",
        "        'hidden_dims': HIDDEN_DIMS,\n",
        "        'paddings': PADDINGS,\n",
        "        'kernel_sizes': KERNEL_SIZES,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()}, \n",
        "        'checkpoint_cnn_1')\n",
        "      best_score = score\n",
        "      \n",
        "    scheduler.step(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsOo8YNgxYCw",
        "colab_type": "text"
      },
      "source": [
        "# Инференс (все 3 модели)\n",
        "(около 8 минут на колабе)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOuI9RJe63Im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('kontur_srs_test_task/train_data.tsv', sep='\\t')\n",
        "test = pd.read_csv('kontur_srs_test_task/test_data.tsv', sep='\\t')\n",
        "\n",
        "def transform(string):\n",
        "  rep = {'<<': '\"', \n",
        "         '>>': '\"',\n",
        "         '<': '\"', \n",
        "         '>': '\"', \n",
        "         \"''\": '\"', \n",
        "         '``': '\"',\n",
        "         '`': \"'\", \n",
        "         '/': ' ', \n",
        "         '«': '\"', \n",
        "         '»': '\"',\n",
        "         '–': '-',\n",
        "         '_': '-'}\n",
        "  rep = dict((re.escape(k), v) for k, v in rep.items()) \n",
        "\n",
        "  pattern = re.compile(\"|\".join(rep.keys()))\n",
        "  replaced = pattern.sub(lambda m: rep[re.escape(m.group(0))], string)\n",
        "  filtered = ''.join(re.findall(\"[A-Za-z0-9а-яА-Я\\-',.\\(\\)\\\" &@!?\\+\\*№]*\", replaced))\n",
        "  if filtered.strip() == '':\n",
        "    # empty string token\n",
        "    filtered = '#'\n",
        "  return filtered\n",
        "\n",
        "test_transformed = utils.transform_dataframe(test, transform)\n",
        "\n",
        "all_preds = np.zeros((len(test), 3))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for i in range(1, 4):\n",
        "  tokenizer = yttm.BPE(model='models/tokenizer_320cased_{}.model'.format(i))\n",
        "  cp = torch.load('models/checkpoint_cnn_{}'.format(i))\n",
        "\n",
        "  SEED = cp['seed']\n",
        "  VOCAB_SIZE = cp['vocab_size']\n",
        "  EMBEDDING_DIM = cp['embedding_dim']\n",
        "  HIDDEN_DIMS = cp['hidden_dims']\n",
        "  KERNEL_SIZES = cp['kernel_sizes']\n",
        "  PADDINGS = cp['paddings']\n",
        "  DROPOUT = cp['dropout']\n",
        "\n",
        "  _, thresh_split, _ = utils.split_train(train, 'eng_name', thresh_size=0.4, seed=SEED)\n",
        "\n",
        "  thresh_split = utils.transform_dataframe(thresh_split, transform)\n",
        "  threshold_lists = utils.precompute_dataset(thresh_split, tokenizer)\n",
        "  test_lists = utils.precompute_dataset(test_transformed, tokenizer)\n",
        "\n",
        "  threshold_loader = DataLoader(utils.ListsDataset(*threshold_lists), batch_size=200, collate_fn=utils.collate_fn)\n",
        "  test_loader = DataLoader(utils.ListsDataset(*test_lists), batch_size=200, collate_fn=lambda batch: utils.collate_fn(batch, with_labels=False))\n",
        "\n",
        "  model = SiameseCNNEncoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIMS, KERNEL_SIZES, PADDINGS, DROPOUT)\n",
        "  model.to(device)\n",
        "\n",
        "  model.load_state_dict(cp['model_state_dict'])\n",
        "\n",
        "  threshold_tuner = utils.OptimizedRounder(0.73)\n",
        "\n",
        "  similarities = []\n",
        "  trues = []\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in threshold_loader:\n",
        "      ru, eng, labels = batch\n",
        "      trues.extend(list(labels.numpy()))\n",
        "      ru, eng = ru.to(device), eng.to(device)\n",
        "      similarities.extend(list(model.compute_similarity(ru, eng).cpu().numpy()))\n",
        "\n",
        "  threshold_tuner.fit(similarities, trues)\n",
        "\n",
        "  similarities = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "      ru, eng = batch\n",
        "      ru, eng = ru.to(device), eng.to(device)\n",
        "      similarities.extend(list(model.compute_similarity(ru, eng).cpu().numpy()))\n",
        "  \n",
        "  preds = threshold_tuner.predict(similarities)\n",
        "  all_preds[:, i - 1] = preds\n",
        "\n",
        "\n",
        "final_preds = np.apply_along_axis(lambda preds: np.median(preds).astype(bool), 1, all_preds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blW3I_VMBcKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test['answer'] = final_preds\n",
        "test.to_csv('answers.tsv', sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhESTKwsGUCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}